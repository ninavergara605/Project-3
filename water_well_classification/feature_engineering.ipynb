{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"feature_engineering.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1xBwx-czm6f2cLyux6juMHyGuGZdvP6BO","authorship_tag":"ABX9TyMICJx4R+26aMRCgQlPY7zV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AbstDuHZulBf","executionInfo":{"status":"ok","timestamp":1629203141170,"user_tz":240,"elapsed":2256,"user":{"displayName":"Nina Vergara","photoUrl":"","userId":"05317247971731800776"}},"outputId":"73580b8c-b85b-48f9-82f4-b2831a703132"},"source":["import pandas as pd\n","import numpy as np\n","\n","from sklearn.preprocessing import OneHotEncoder\n","from itertools import chain\n","from functools import reduce\n","\n","\n","\n","\n","train_height_imp = pd.read_csv('/data/cleaned_X_train_dist_imputed.csv')\n","test_height_imp = pd.read_csv('/data/cleaned_X_test_dist_imputed.csv')\n","y = pd.read_csv('/data/y_train_resamp.csv')\n","train_height_imp['y'] = y['functional_group']\n","train_height_imp['id'] = np.arange(len(train_height_imp))\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CgJmw1m-VMW7"},"source":["# Feature Creation"]},{"cell_type":"markdown","metadata":{"id":"FNmpV3n2VRGj"},"source":["## Bin Construction Year\n","Bin the construction years into intervals which span 3 years"]},{"cell_type":"code","metadata":{"id":"FbXV1Zsmuz_p"},"source":["def bin_construct_year(df):\n","  # Bins the construction year into 3 year intervals\n","  \n","  # Hardocde 1963 because some rows have a construction year of '0' \n","  year_bins = pd.interval_range(1963,df['construction_year'].max(),freq=3)\n","  df['construct_year_bin'] = pd.cut(df['construction_year'], year_bins)\n","\n","  # Creates labels indicating the construction year to replace the raw interval dtype\n","  labels = [f'{str(bin.left)}-{str(bin.right)}' for bin in year_bins]\n","  df['construct_year_bin']  = df['construct_year_bin'].map(dict(zip(year_bins,labels))).astype(str, errors='ignore')\n","  # Fills any rows that weren't binned (like those equal to '0') as missing\n","  df.loc[df['construct_year_bin'].isna(), 'construct_year_bin'] = 'missing'\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wy0ZWNtaJHVT"},"source":["## Create KNN columns"]},{"cell_type":"markdown","metadata":{"id":"MqvoNWwDJNU0"},"source":["For each point in the training data:\n"," \n","\n","1.   Find the nearest neighbors using 3-dim Euclidean distance\n","2.  Determine percentage of neighbors that have a certain condition for each point\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"KH0RfMvgxba7","executionInfo":{"status":"ok","timestamp":1629203146277,"user_tz":240,"elapsed":514,"user":{"displayName":"Nina Vergara","photoUrl":"","userId":"05317247971731800776"}}},"source":["def cross_join_dfs(df_1, df_2, suffixes=['_test', '_cond_probs']):\n","    # Pair every row in df_1 with every row in df_2, like a cross join in SQL or cartesian product\n","    df_1['merge_key'], df_2['merge_key'] = 1,1\n","\n","    cross_joined = df_1.merge(df_2,on='merge_key', suffixes=suffixes).drop(columns='merge_key')\n","    return cross_joined\n","\n","def filter_dfs(dfs, filter_for):\n","  filtered = [df[df.columns[df.columns.isin(filter_for)]].copy() \n","                          for df in dfs]\n","  return filtered                          \n","\n","def merge_X_neighbor_cols(X_neighbor, X):\n","  if not 'id' in X.columns:\n","    X['id'] = np.arange(len(X))\n","  combined = X.iloc[:750].merge(X_neighbor.reset_index()[['func_neighbor_perc'\n","                                                ,'non_func_neighbor_perc'\n","                                                ,'needs_repair_neighbor_perc'\n","                                                ,'id']]\n","                                                  ,on='id')\n","  return combined"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z-1gUWzotzsQ"},"source":["def calc_y_prop(group,k):\n","  # Find the K nearest points and counts the number of times our target values appear\n","  \n","  y_prop = group.nlargest(k, 'distance')['y'].value_counts()/k\n","\n","  return y_prop.reset_index()  \n","  \n","  \n","\n","def calc_distance(df):\n","  # Compute the distances between the selected point and all points in X Train\n","  df['distance'] = np.sqrt((df['latitude_1']-df['latitude'])**2  \n","                                    +(df['longitude_1']-df['longitude'])**2  \n","                                    +(df['gps_height_1']-df['gps_height'])**2)\n","  \n","  # Remove the points that are in the same exact location of our isolated point\n","  df = df[df['distance']!=0]\n","  return df\n","\n","def reformat(dfs, X):\n","  df = pd.concat(dfs)\n","  df.columns = ['y_pred', 'proportion'] \n","  df = df.reset_index(level='id')\n","  df['neighbor_cols'] = df['y_pred'].map({'functional': 'func_neighbor_perc'\n","                                                ,'non functional': 'non_func_neighbor_perc'\n","                                                ,'functional needs repair': 'needs_repair_neighbor_perc'})\n","  res = df.pivot(values='proportion',index='id', columns='neighbor_cols').fillna(0)\n","  return merge_X_neighbor_cols(res,X)\n","\n","def get_knn_conditions(X_train,k):\n","    X_train['id'] = np.arange(len(X_train))\n","    # Filters X_train_neighbors and X_test for essential columns to lighten the RAM load\n","    filter_for = ['func_neighbor_perc'\n","                  ,'non_func_neighbor_perc'\n","                  ,'needs_repair_neighbor_perc'\n","                ,'latitude'\n","                ,'longitude'\n","                ,'id'\n","                ,'y'\n","                ,'gps_height']\n","    filtered_train = filter_dfs([X_train], filter_for)[0]\n","    \n","    # Separates X_test into batches and dispatches them, along with all X_neighbor points \n","    processed = []\n","    # Filter out duplicate values created with oversampling\n","    filt_no_dupl = filtered_train.drop_duplicates(subset=['latitude', 'longitude', 'gps_height']).copy()\n","    batch_indexes = pd.interval_range(0,len(X_train), freq = 250)\n","    for interval in batch_indexes:\n","        train_batch = filtered_train.iloc[interval.left:interval.right,:].copy()\n","        cross_joined = cross_join_dfs(train_batch, filt_no_dupl, suffixes=['', '_1'])\n","        with_distance = calc_distance(cross_joined)\n","        \n","        res = with_distance.groupby('id').apply(calc_y_prop, k)\n","        processed.append(res)\n","\n","    res = reformat(processed, X_train)\n","    return res\n","\n","k=5\n","X_train_with_neighbor = get_knn_conditions(train_height_imp,k)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0OfnCF7UmvTF","executionInfo":{"status":"ok","timestamp":1629171973193,"user_tz":240,"elapsed":168,"user":{"displayName":"Nina Vergara","photoUrl":"","userId":"05317247971731800776"}},"outputId":"a8be5123-aa2b-47d7-e875-d3e5dcf8021d"},"source":["def calc_precision(df,value):\n","  num = len(df[(df['y'] == 'functional') & (df['pred_y'] == 'functional')])\n","  denom = len(df[df['pred_y'] == 'functional'])\n","  return  num/denom  \n","def get_pred(df):\n","  df.loc[(df['func_neighbor_perc'] > df['non_func_neighbor_perc']) & (df['func_neighbor_perc'] > df['needs_repair_neighbor_perc']), 'pred_y'] = 'functional'\n","  df.loc[(df['func_neighbor_perc'] < df['non_func_neighbor_perc']) & (df['non_func_neighbor_perc'] > df['needs_repair_neighbor_perc']), 'pred_y'] = 'non functional'\n","  df.loc[(df['func_neighbor_perc'] < df['needs_repair_neighbor_perc']) & (df['non_func_neighbor_perc'] < df['needs_repair_neighbor_perc']), 'pred_y'] = 'functional needs repair'\n","  print(f\"Precision: functional {calc_precision(df,'functional')} non functional {calc_precision(df,'non functional')} needs_repair {calc_precision(df,'functional needs repair')}\")\n","\n","get_pred(neighbor_data)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Precision: functional 1.0 non functional 1.0 needs_repair 1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tO_P_o7yjXS3"},"source":["def find_closest_train_perc(test_data, cond_probs):\n","    joined = cross_join_dfs(test_data, cond_probs)\n","    \n","    # Compute the euclidean distance between the X_test_point and X_train_point pairs\n","    joined['distance'] = np.sqrt((joined['latitude_test']-joined['latitude_cond_probs'])**2  \n","                                      +(joined['longitude_test']-joined['longitude_cond_probs'])**2  \n","                                      +(joined['gps_height_test']-joined['gps_height_cond_probs'])**2)\n","    \n","    # For each test id, return the pairing with the closest distance\n","    joined_filt = joined.loc[joined.groupby('id')['distance'].idxmin()]\n","    return joined_filt\n","\n","def get_test_neighbor_perc(X_neighbor, X_test):\n","    # Decided on a vectorized batch processing approach because get_knn_cond was EXTREMELY slow\n","    \n","    # Filters X_train_neighbors and X_test for essential columns to lighten the RAM load\n","    filter_for = ['func_neighbor_perc'\n","                  ,'non_func_neighbor_perc'\n","                  ,'needs_repair_neighbor_perc'\n","                ,'latitude'\n","                ,'longitude'\n","                ,'id'\n","                ,'gps_height']\n","    X_neighbor_filt, X_test_filt = filter_dfs(X_neighbor, X_test, filter_for)\n","    \n","    # Filter out duplicate values created with oversampling\n","    X_neighbor_filt = X_neighbor_filt.drop_duplicates()\n","    \n","    # Separates X_test into batches and dispatches them, along with all X_neighbor points \n","    processed = []\n","    batch_indexes = pd.interval_range(0,len(X_test_filt), freq = 350)\n","    for interval in batch_indexes:\n","        X_test_batch = X_test_filt.iloc[interval.left:interval.right,:].copy()\n","        processed.append(find_closest_train_perc(X_test_batch, X_neighbor_filt))\n","        \n","    # Need to drop the NaN rows that overindexing with iloc creates \n","    full_df = pd.concat(processed).dropna()\n","    return full_df\n","\n","test_neighbor_data = get_test_neighbor_perc(X_train_with_neighbor, X_test_raw)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cci_506TKKm8"},"source":["X_test_with_neighbor = merge_X_neighbor_cols(X_test_with_neighbor, test_height_imp)\n","#X_test = bin_construct_year(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1cRPbn2ar2Er"},"source":["# OHE and Dropping Features"]},{"cell_type":"code","metadata":{"id":"HXKslLtRbIu2"},"source":["cols_to_keep = [\n","              ,'water_quality'\n","              ,'waterpoint_type_group'\n","              ,'source'\n","              ,'quantity'\n","              ,'quality_group'\n","              ,'water_quality'\n","              ,'payment_type'\n","              ,'management'\n","              ,'extraction_type'\n","              ,'amount_tsh'\n","              ,'permit'\n","              ,'public_meeting'\n","              ,'func_neighbor_perc'\n","              ,'non_func_neighbor_perc'\n","              ,'needs_repair_neighbor_perc'\n","              ,'id'\n","              ,'y']\n","train_filt, X_test_filt = filter_dfs([X_train_with_neighbor, X_test_with_neighbor], cols_to_keep)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6_E0__807eH9"},"source":["# Using this to ensure that any one hot encoded 'missing' values are properly differentiated \n","\n","def add_suffix(col, value):\n","  name = col.name\n","  new_value = value + '_'+ name\n","  col[col == value] = new_value\n","  return col"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FKBeP-aJXfSQ"},"source":["def separate_train_data(df):\n","    df['id'] = np.arange(len(df))\n","    y_actual = df[['y', 'id']]\n","    X = df.drop(columns='y')\n","    return y_actual, X\n","\n","y_train, X_train = separate_train_data(train_filt)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M5RfIC9I6sR0"},"source":["def one_hot_encode(fit_df, transform_df, target_cols):\n","  target_fit, target_transform = [df[target_cols].apply(add_suffix, args=('missing',)) for df in [fit_df, transform_df]]\n","  ohe = OneHotEncoder(sparse=False).fit(target_fit)\n","  \n","  transformed = ohe.transform(target_transform)\n","  cols = list(chain(*ohe.categories_))\n","  res = pd.DataFrame(transformed, columns =cols)\n","  return res\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G6T05xVm1Uz7"},"source":["def ohe_dispatch(train, test):\n","    cat_cols = train.select_dtypes(include='object').columns\n","\n","    test_ohe = one_hot_encode(train, test, cat_cols)\n","    train_ohe = one_hot_encode(train, train, cat_cols)\n","\n","    full_train = pd.concat([train, train_ohe], axis='columns').drop(columns=cat_cols)\n","    full_test = pd.concat([test, test_ohe], axis='columns').drop(columns=cat_cols)\n","    return full_train, full_test\n","    \n","full_train, full_test  = ohe_dispatch(X_train, X_test_filt)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MRPFOAvpDPGt"},"source":["full_train.to_csv('/data/modeling_data_train.csv')\n","full_test.to_csv('/data/modeling_data_test.csv')"],"execution_count":null,"outputs":[]}]}